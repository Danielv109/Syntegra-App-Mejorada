# SYNTEGRA - Plataforma de Inteligencia de Clientes

## ğŸš€ DescripciÃ³n

SYNTEGRA es una plataforma completa de inteligencia de datos empresariales que funciona completamente **offline** o en entornos locales, sin depender de servicios externos.

## âœ¨ CaracterÃ­sticas Principales

- **ETL Automatizado**: Ingesta y limpieza de datos estructurados (CSV, Excel, JSON)
- **Data Connectors**: Sistema de conectores para fuentes externas
- **Data Processing**: Limpieza y normalizaciÃ³n automÃ¡tica de datos
- **AnÃ¡lisis de Texto con IA Local**: AnÃ¡lisis de sentimiento y extracciÃ³n de keywords usando Ollama
- **DetecciÃ³n de Tendencias**: IdentificaciÃ³n automÃ¡tica de patrones emergentes
- **DetecciÃ³n de AnomalÃ­as**: Usando IsolationForest de scikit-learn
- **Clustering de Clientes**: AgrupaciÃ³n inteligente de empresas similares
- **GeneraciÃ³n de Reportes**: Informes PDF automÃ¡ticos con mÃ©tricas clave
- **Gold Dataset**: Sistema de aprendizaje continuo con correcciones humanas
- **Multi-tenant**: Soporte para mÃºltiples clientes con aislamiento de datos
- **Procesamiento AsÃ­ncrono**: Workers con Celery para tareas pesadas

## ğŸ“Š MÃ³dulos Principales

### 1. Data Connectors

- ConfiguraciÃ³n de fuentes de datos externas
- ValidaciÃ³n basada en templates YAML
- EjecuciÃ³n asÃ­ncrona de ingestas
- Historial completo de operaciones

### 2. Data Processing

- Limpieza automÃ¡tica de texto (HTML, emojis, caracteres especiales)
- NormalizaciÃ³n de fechas (mÃºltiples formatos)
- Normalizadores especÃ­ficos por tipo (restaurant, retail, service)
- Almacenamiento en tabla `processed_data`

### 3. Text Analysis

- AnÃ¡lisis de sentimiento con Ollama (IA local)
- ExtracciÃ³n de keywords con spaCy
- GeneraciÃ³n de embeddings
- DetecciÃ³n de entidades

### 4. Anomaly Detection

- IsolationForest (mÃ©todo principal)
- EllipticEnvelope (multivariado)
- LocalOutlierFactor (densidad local)
- Ensemble de mÃ©todos

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Backend**: Python 3.11 + FastAPI
- **Base de Datos**: PostgreSQL + pgvector
- **Cache/Queue**: Redis
- **IA Local**: Ollama (phi3:mini, mistral, llama3)
- **NLP**: spaCy + sentence-transformers
- **ML**: scikit-learn + pandas
- **Async**: Celery
- **Reports**: ReportLab
- **Container**: Docker + Docker Compose

## ğŸ“‹ Requisitos

- Docker y Docker Compose
- Python 3.11+
- Ollama instalado localmente
- RAM mÃ­nima: 8 GB
- Espacio en disco: 20 GB

## ğŸ”§ InstalaciÃ³n

### 1. Clonar repositorio

```bash
git clone <repository-url>
cd Syntegra-App-Mejorada
```

### 2. Configurar variables de entorno

```bash
cp .env.example .env
# Editar .env con tus valores
```

### 3. Instalar Ollama (si no estÃ¡ instalado)

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Descargar desde https://ollama.com/download
```

### 4. Descargar modelo de Ollama

```bash
ollama pull phi3:mini
```

### 5. Iniciar servicios con Docker

```bash
docker-compose up -d
```

### 6. Crear usuario administrador

```bash
docker-compose exec api python scripts/init_admin.py
```

### 7. (Opcional) Crear datos de ejemplo

```bash
docker-compose exec api python scripts/create_sample_data.py
```

## ğŸ“š DocumentaciÃ³n API

Una vez iniciada la aplicaciÃ³n, visita:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### DocumentaciÃ³n Detallada

- [Data Connectors](DOCS/CONNECTORS.md)
- [Data Processing](DOCS/DATA_PROCESSING.md)

## ğŸ” AutenticaciÃ³n

### Login

```bash
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "username": "admin",
    "password": "Admin123!"
  }'
```

### Usar Token

```bash
curl -X GET http://localhost:8000/auth/me \
  -H "Authorization: Bearer <your-token>"
```

## ğŸ“Š Flujo de Trabajo BÃ¡sico

### 1. Crear Cliente

```bash
curl -X POST http://localhost:8000/auth/clients \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Empresa Demo",
    "industry": "TecnologÃ­a"
  }'
```

### 2. Subir Dataset

```bash
curl -X POST http://localhost:8000/datasets/upload \
  -H "Authorization: Bearer <token>" \
  -F "file=@ventas.csv" \
  -F "name=Ventas Q1" \
  -F "description=Datos de ventas primer trimestre"
```

### 3. Analizar Texto

```bash
curl -X POST http://localhost:8000/analysis/datasets/1/analyze-text \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "text_columns": ["comentarios", "descripcion"]
  }'
```

### 4. Generar Reporte

```bash
curl -X POST http://localhost:8000/reports/generate \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{
    "weights": {
      "satisfaccion": 1.5,
      "ventas": 1.2,
      "retencion": 1.0
    }
  }'
```

## ğŸ§ª Testing

```bash
# Todos los tests
pytest

# Tests por mÃ³dulo
make test-connectors
make test-processing

# Con coverage
make test-processing-coverage

# Demos interactivos
make demo-processing
```

## ğŸ“ Estructura del Proyecto

```bash
app/
â”œâ”€â”€ api.py               # Archivo principal de la API
â”œâ”€â”€ models.py            # Modelos de datos y esquemas Pydantic
â”œâ”€â”€ services.py         # LÃ³gica de negocio y servicios
â”œâ”€â”€ tasks.py             # Tareas de Celery
â”œâ”€â”€ connectors/          # MÃ³dulo de Data Connectors
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py        # Modelos especÃ­ficos de conectores
â”‚   â”œâ”€â”€ schemas.py       # Esquemas Pydantic para validaciÃ³n
â”‚   â””â”€â”€ tasks.py         # Tareas de ingesta y conexiÃ³n
â”œâ”€â”€ processing/          # MÃ³dulo de Data Processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py        # Modelos para procesamiento de datos
â”‚   â”œâ”€â”€ schemas.py       # Esquemas Pydantic para validaciÃ³n
â”‚   â””â”€â”€ tasks.py         # Tareas de procesamiento y normalizaciÃ³n
â”œâ”€â”€ analysis/            # MÃ³dulo de AnÃ¡lisis de Datos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py        # Modelos para anÃ¡lisis de datos
â”‚   â”œâ”€â”€ schemas.py       # Esquemas Pydantic para validaciÃ³n
â”‚   â””â”€â”€ tasks.py         # Tareas de anÃ¡lisis y generaciÃ³n de reportes
â”œâ”€â”€ db.py                # ConfiguraciÃ³n de la base de datos y modelos SQLAlchemy
â”œâ”€â”€ main.py              # Punto de entrada de la aplicaciÃ³n
â””â”€â”€ settings.py          # ConfiguraciÃ³n general de la aplicaciÃ³n
```
